\section{Introducción}
La representación densa de información, como el texto, no reside en un espacio plano, sino en un espacio de alta dimensión. Es común suponer que los puntos de tales "modelos semánticos del mundo" tienden a agruparse en una variedad, asumiendo ciertas propiedades, tanto locales como globales. ¿Cuál es la idea? ¿Cómo podemos usarla? La idea es medir y utilizar propiedades locales para, por ejemplo, identificar desiertos semánticos o, por el contrario, regiones de alta concentración. Esto podría ayudar a los algoritmos de aprendizaje.

En este trabajo, exploramos la hipótesis de que modelar explícitamente el espacio de embeddings como una variedad Riemanniana deformable puede mejorar la recuperación de información. Presentamos un modelo que aprende a deformar el espacio latente para reflejar mejor las relaciones semánticas entre documentos y consultas.
