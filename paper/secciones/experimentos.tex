\section{Evaluación y Experimentos}
Para validar nuestro enfoque, diseñamos una serie de experimentos.

\subsection{Evaluación centrada en el LLM}
Usamos métricas como \textbf{RARE} (¿cuánto mejora la generación del LLM con tus docs?) y \textbf{SUD} (¿los docs que recuperas pero no etiquetados ayudan realmente?) para cerrar el bucle: \textbf{lo que importa es mejorar la respuesta generativa}, no sólo el recall.

\section{Experiments}
\label{sec:experiments}

We evaluate our approach on standard information retrieval benchmarks, comparing against strong baselines and analyzing the impact of our geometric regularization.

\subsection{Datasets}

We conduct experiments on the following datasets:

\begin{itemize}
    \item \textbf{MS MARCO Passage Ranking}: Large-scale web search dataset with 8.8M passages and 1M training queries
    \item \textbf{BEIR}: A heterogeneous benchmark containing 18 datasets across various domains
    \item \textbf{Natural Questions}: Open-domain QA dataset with real user queries
\end{itemize}

For computational efficiency, we use the dev set of MS MARCO (6,980 queries) for validation and report results on the BEIR benchmark for zero-shot evaluation.

\subsection{Baselines}

We compare against the following strong baselines:

\begin{itemize}
    \item \textbf{BM25}: Traditional lexical retrieval method
    \item \textbf{DPR}: Dense passage retriever with BERT-base
    \item \textbf{ANCE}: State-of-the-art dense retriever with hard negative mining
    \item \textbf{ColBERT}: Late interaction model with token-level matching
    \item \textbf{Contriever}: Unsupervised contrastive learning baseline
\end{itemize}

\subsection{Implementation Details}

\begin{itemize}
    \item All models use BERT-base architecture with 110M parameters
    \item Training: 3 epochs with batch size 128, learning rate 2e-5
    \item Maximum sequence length: 512 tokens
    \item Embedding dimension: 768
    \item k-NN graph built with k=30 for curvature computation
    \item Training conducted on 4×A100 GPUs (40GB)
\end{itemize}

\subsection{Evaluation Metrics}

We employ both traditional IR metrics and our proposed LLM-aware metrics:

\textbf{Traditional Metrics:}
\begin{itemize}
    \item nDCG@k (Normalized Discounted Cumulative Gain)
    \item MAP (Mean Average Precision)
    \item Recall@k
    \item MRR (Mean Reciprocal Rank)
\end{itemize}

\textbf{Proposed Metrics:}
\begin{itemize}
    \item \textbf{RARE@k}: Response quality using GPT-4 as judge
    \item \textbf{SUD}: Semantic utility of non-groundtruth documents
    \item \textbf{Contradiction Resilience}: Measures robustness to contradictory information
\end{itemize}

\subsection{Training Protocol}

\begin{enumerate}
    \item Pretrain the base encoder on MS MARCO using standard contrastive loss
    \item Fine-tune with our proposed geometric objectives
    \item For each batch:
    \begin{itemize}
        \item Construct k-NN graph
        \item Compute geodesic distances
        \item Update model parameters using combined loss
    \end{itemize}
    \item Evaluate on validation set every 1,000 steps
    \item Early stopping with patience of 3 evaluations
\end{enumerate}

\subsection{Hyperparameter Tuning}

We perform grid search over the following hyperparameters:

\begin{itemize}
    \item Learning rate: \{1e-5, 2e-5, 5e-5\}
    \item Temperature $\tau$: \{0.01, 0.05, 0.1\}
    \item Curvature weight $\lambda$: \{0.1, 0.5, 1.0\}
    \item Neighborhood size k: \{10, 30, 50\}
\end{itemize}

Selected values are chosen based on performance on the MS MARCO dev set.

\subsection{Configuración Experimental}
(Detalles sobre los datasets, modelos base, hiperparámetros, etc.)

\subsection{Resultados Cuantitativos}
(Tablas con resultados de RARE, SUD, y otras métricas tradicionales como nDCG, Recall@K, etc.)
