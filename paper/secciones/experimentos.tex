\section{Evaluación y Experimentos}
Para validar nuestro enfoque, diseñamos una serie de experimentos.

\subsection{Evaluación centrada en el LLM}
Usamos métricas como \textbf{RARE} (¿cuánto mejora la generación del LLM con tus docs?) y \textbf{SUD} (¿los docs que recuperas pero no etiquetados ayudan realmente?) para cerrar el bucle: \textbf{lo que importa es mejorar la respuesta generativa}, no sólo el recall.

\subsection{Configuración Experimental}
(Detalles sobre los datasets, modelos base, hiperparámetros, etc.)

\subsection{Resultados Cuantitativos}
(Tablas con resultados de RARE, SUD, y otras métricas tradicionales como nDCG, Recall@K, etc.)
