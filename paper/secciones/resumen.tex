La recuperación de información tradicionalmente opera en espacios de embeddings planos. Proponemos un nuevo enfoque donde el espacio latente es modelado como una variedad Riemanniana deformable, cuya métrica es aprendida de forma end-to-end. Este enfoque permite una noción más rica de "distancia semántica" a través de geodésicas. Mediante una pérdida contrastiva geométrica y una regularización de la curvatura, enseñamos al modelo a construir un espacio que mejora significativamente la recuperación de documentos para tareas de generación de lenguaje, como lo demuestran nuestras evaluaciones con métricas centradas en el LLM como RARE y SUD.
