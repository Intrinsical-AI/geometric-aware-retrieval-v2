We present a novel approach to learning retrieval embeddings by modeling the embedding space as a learnable Riemannian manifold. Our method introduces geometric inductive biases through curvature regularization, enabling the model to better capture the complex, non-uniform semantic structures present in natural language. The proposed framework combines contrastive learning on geodesic distances with Ricci-Ollivier curvature regularization to shape the embedding space according to the underlying data manifold. We introduce two novel evaluation metrics, Response-Aware Retrieval Effectiveness (RARE) and Semantic Utility Delta (SUD), which leverage large language models to assess retrieval quality in terms of downstream task performance. Extensive experiments on standard benchmarks demonstrate that our approach outperforms strong baselines, with particular improvements in low-density semantic regions. The results suggest that explicitly modeling the geometric properties of the embedding space can lead to more effective and robust retrieval systems, especially when combined with modern language model capabilities.
