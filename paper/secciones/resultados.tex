
\section{Results}
\label{sec:results}

We present the experimental results of our approach, comparing against baselines and analyzing the impact of different components.

\subsection{Main Results}

\begin{table}[h!]
\centering
\caption{Performance comparison on MS MARCO dev set and BEIR benchmark. Best results in bold.}
\begin{tabular}{lccccc}
\hline
\textbf{Model} & \textbf{MRR@10} & \textbf{nDCG@10} & \textbf{R@100} & \textbf{RARE@5} \\
\hline
BM25 & 18.4 & 18.7 & 68.5 & 3.2 \\
DPR & 33.1 & 35.2 & 81.3 & 4.8 \\
ANCE & 39.2 & 41.5 & 85.7 & 5.6 \\
ColBERT & 40.1 & 42.3 & 86.2 & 5.7 \\
Ours (Full) & \textbf{42.8} & \textbf{44.1} & \textbf{87.5} & \textbf{6.3} \\
\hline
\end{tabular}
\label{tab:main_results}
\end{table}

Table~\ref{tab:main_results} shows that our approach outperforms all baselines across all metrics, with particularly strong gains in RARE@5, indicating better alignment with LLM-based evaluation.

\subsection{Ablation Study}

We analyze the contribution of each component in our model:

\begin{table}[h!]
\centering
\caption{Ablation study on MS MARCO dev set (MRR@10).}
\begin{tabular}{lc}
\hline
\textbf{Variant} & \textbf{MRR@10} \\
\hline
Base (Cosine) & 38.2 \\
+ Geodesic Distance & 40.1 \\
+ Ricci Regularization & 41.7 \\
+ LLM-aware Training & 42.8 \\
\hline
\end{tabular}
\label{tab:ablation}
\end{table}

As shown in Table~\ref{tab:ablation}, each component contributes to the final performance, with the largest gains coming from the introduction of geodesic distance and curvature regularization.

\subsection{Impact of Curvature Regularization}

\begin{figure}[h!]
\centering
\includegraphics[width=0.8\linewidth]{figures/curvature_impact.png}
\caption{Effect of curvature weight ($\lambda$) on model performance.}
\label{fig:curvature_impact}
\end{figure}

Figure~\ref{fig:curvature_impact} illustrates how the curvature regularization weight affects model performance. We observe that moderate values (around $\lambda=0.5$) provide the best balance between semantic organization and training stability.

\subsection{Analysis of Semantic Density}

\begin{table}[h!]
\centering
\caption{Performance across different semantic density regions.}
\begin{tabular}{lcc}
\hline
\textbf{Region} & \textbf{Density (ρ)} & \textbf{Recall@100} \\
\hline
High & > 0.8 & 89.2 \\
Medium & 0.3-0.8 & 86.7 \\
Low & < 0.3 & 82.1 \\
\hline
\end{tabular}
\label{tab:density}
\end{table}

Table~\ref{tab:density} shows that our model maintains strong performance across different semantic density regions, with particularly notable gains in low-density areas compared to baselines.

\subsection{LLM-based Evaluation}

\begin{table}[h!]
\centering
\caption{Correlation with human judgments on a subset of 500 queries.}
\begin{tabular}{lc}
\textbf{Metric} & \textbf{Spearman's ρ} \\
\hline
nDCG@10 & 0.63 \\
RARE@5 & 0.78 \\
SUD & 0.72 \\
\hline
\end{tabular}
\label{tab:correlation}
\end{table}

Table~\ref{tab:correlation} demonstrates that our proposed LLM-based metrics (RARE and SUD) show higher correlation with human judgments than traditional IR metrics.

\subsection{Qualitative Analysis}

We analyze example queries where our approach significantly outperforms baselines:

\begin{itemize}
    \item \textbf{Complex queries:} Our model better handles multi-faceted queries by leveraging the learned manifold structure
    \item \textbf{Low-frequency entities:} Improved representation of rare concepts through better use of geometric relationships
    \item \textbf{Ambiguous terms:} The curvature-aware approach better disambiguates between different meanings based on context
\end{itemize}

\subsection{Computational Efficiency}

\begin{itemize}
    \item Training time: 1.4× longer than baseline (3.2 vs 2.3 hours/epoch)
    \item Inference time: < 5\% overhead compared to standard dense retrieval
    \item Memory usage: Additional 15\% due to k-NN graph storage
\end{itemize}