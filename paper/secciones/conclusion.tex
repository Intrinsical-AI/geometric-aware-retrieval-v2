\section{Conclusión}
En conjunto, nuestro trabajo demuestra que aprender \textbf{de forma end-to-end} una variedad y su métrica, recuperar documentos por \textbf{geodésicas} en ese espacio y validar que \textbf{esa geometría realmente potencia} la generación de respuestas, transforma la metáfora del “cartógrafo” en un sistema operativo concreto. Este enfoque abre nuevas vías para la investigación en recuperación de información y la comprensión de los espacios de representación neuronal.

\section{Conclusion}
\label{sec:conclusion}

We have presented a novel approach to learning retrieval embeddings that explicitly models the embedding space as a learnable Riemannian manifold. Our method combines geometric deep learning principles with modern information retrieval techniques, demonstrating significant improvements over traditional approaches.

\subsection{Key Contributions}

Our work makes several important contributions to the field of neural information retrieval:

\begin{itemize}
    \item \textbf{Geometric Framework}: We introduced a principled approach to learning retrieval embeddings by treating the embedding space as a Riemannian manifold with learnable curvature.
    
    \item \textbf{Curvature-Aware Training}: The proposed Ricci-Ollivier curvature regularization helps shape the embedding space to better reflect semantic relationships, particularly in low-density regions.
    
    \item \textbf{LLM-Aware Evaluation}: We proposed RARE and SUD metrics that better align retrieval quality with downstream LLM performance.
    
    \item \textbf{Empirical Validation}: Comprehensive experiments on standard benchmarks demonstrate the effectiveness of our approach, with consistent improvements over strong baselines.
\end{itemize}

\subsection{Limitations}

While our approach shows promising results, it has several limitations that suggest directions for future work:

\begin{itemize}
    \item The computational overhead of computing geodesic distances and curvature, though mitigated by our optimizations, remains non-trivial.
    
    \item The current implementation assumes a static embedding space, while real-world applications often require handling of streaming or dynamic document collections.
    
    \item The effectiveness of our approach depends on the quality of the initial embedding space, suggesting potential benefits from curriculum learning approaches.
\end{itemize}

\subsection{Future Work}

Several promising directions for future research emerge from this work:

\begin{itemize}
    \item \textbf{Dynamic Manifolds}: Extend the framework to handle temporal dynamics in document collections.
    
    \item \textbf{Hierarchical Curvature}: Investigate multi-scale curvature modeling to capture both local and global semantic structures.
    
    \item \textbf{Cross-Modal Retrieval}: Apply the geometric framework to cross-modal retrieval tasks where the relationship between modalities can be naturally modeled through manifold learning.
    
    \item \textbf{Efficient Approximations}: Develop more efficient methods for computing geodesic distances and curvature to enable scaling to even larger collections.
\end{itemize}

\subsection{Final Remarks}

Our work bridges the gap between geometric deep learning and information retrieval, demonstrating that explicitly modeling the embedding space as a learnable manifold can lead to significant improvements in retrieval quality. The proposed LLM-aware evaluation metrics provide a more nuanced understanding of retrieval performance in the context of modern language model applications. We believe this work opens up exciting new directions for research at the intersection of geometry, deep learning, and information retrieval.
